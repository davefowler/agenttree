<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>AgentTree Voice</title>
    <link rel="stylesheet" href="/static/styles.css?v=10">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            background: #1a1a1a;
            color: #e0e0e0;
            height: 100dvh;
            display: flex;
            flex-direction: column;
            overflow: hidden;
            margin: 0;
        }
        .voice-header {
            padding: 12px 16px;
            display: flex;
            align-items: center;
            justify-content: space-between;
            border-bottom: 1px solid #333;
        }
        .voice-header h1 { font-size: 16px; font-weight: 600; }
        .voice-header a { color: #4a9eff; text-decoration: none; font-size: 14px; }

        .voice-main {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            gap: 24px;
            padding: 24px;
        }
        .voice-status {
            font-size: 15px;
            color: #888;
            text-align: center;
            min-height: 40px;
        }
        .voice-status.active { color: #4aff7a; }
        .voice-status.error { color: #ff4a4a; }

        .voice-btn {
            width: 120px; height: 120px;
            border-radius: 50%;
            border: 3px solid #4a9eff;
            background: transparent;
            color: #4a9eff;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s;
            -webkit-tap-highlight-color: transparent;
        }
        .voice-btn svg { width: 48px; height: 48px; }
        .voice-btn.connected {
            background: #4a9eff;
            color: #fff;
            animation: pulse 2s ease-in-out infinite;
            box-shadow: 0 0 40px rgba(74, 158, 255, 0.4);
        }
        .voice-btn.connecting {
            border-color: #888;
            color: #888;
            animation: pulse 1s ease-in-out infinite;
        }
        .voice-btn:disabled { opacity: 0.3; cursor: default; }
        @keyframes pulse {
            0%, 100% { box-shadow: 0 0 20px rgba(74, 158, 255, 0.2); }
            50% { box-shadow: 0 0 50px rgba(74, 158, 255, 0.5); }
        }

        .voice-log {
            width: 100%;
            max-height: 200px;
            overflow-y: auto;
            font-size: 13px;
            color: #666;
            text-align: center;
        }
        .voice-log .tool-call { color: #4a9eff; }

        .no-key-msg {
            text-align: center;
            padding: 40px 20px;
            color: #888;
            font-size: 15px;
            line-height: 1.6;
        }
        .no-key-msg code {
            background: #333;
            padding: 2px 8px;
            border-radius: 4px;
            color: #e0e0e0;
        }
    </style>
</head>
<body>
    <div class="voice-header">
        <h1>AgentTree Voice</h1>
        <a href="/mobile">Back to Mobile</a>
    </div>

    {% if not has_openai_key %}
    <div class="no-key-msg">
        <p>Voice requires an OpenAI API key.</p>
        <p>Set <code>OPENAI_API_KEY</code> in your environment and restart the server.</p>
    </div>
    {% else %}
    <div class="voice-main">
        <div class="voice-status" id="voice-status">Tap to start a voice session</div>

        <button class="voice-btn" id="voice-btn" onclick="toggleVoice()">
            <svg id="mic-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"/>
                <path d="M19 10v2a7 7 0 0 1-14 0v-2"/>
                <line x1="12" y1="19" x2="12" y2="23"/>
                <line x1="8" y1="23" x2="16" y2="23"/>
            </svg>
            <svg id="stop-icon" viewBox="0 0 24 24" fill="currentColor" style="display:none">
                <rect x="6" y="6" width="12" height="12" rx="2"/>
            </svg>
        </button>

        <div class="voice-log" id="voice-log"></div>
    </div>
    {% endif %}

    <script>
    {% if has_openai_key %}
    const statusEl = document.getElementById('voice-status');
    const btn = document.getElementById('voice-btn');
    const micIcon = document.getElementById('mic-icon');
    const stopIcon = document.getElementById('stop-icon');
    const logEl = document.getElementById('voice-log');

    let pc = null;      // RTCPeerConnection
    let dc = null;      // data channel
    let audioEl = null;  // remote audio element
    let localStream = null;

    // Get the issue number from the URL or from parent mobile page context
    function getCurrentIssueId() {
        const params = new URLSearchParams(window.location.search);
        const issue = params.get('issue');
        if (issue) return parseInt(issue, 10);
        // Check if embedded in mobile page
        if (window.parent !== window) {
            try {
                const parentParams = new URLSearchParams(window.parent.location.search);
                return parseInt(parentParams.get('issue'), 10) || null;
            } catch(e) { return null; }
        }
        return null;
    }

    function log(msg, className) {
        const div = document.createElement('div');
        div.textContent = msg;
        if (className) div.className = className;
        logEl.appendChild(div);
        logEl.scrollTop = logEl.scrollHeight;
    }

    async function toggleVoice() {
        if (pc) {
            stopVoice();
        } else {
            await startVoice();
        }
    }

    async function startVoice() {
        statusEl.textContent = 'Connecting...';
        statusEl.className = 'voice-status';
        btn.className = 'voice-btn connecting';

        try {
            // 1. Get ephemeral token from our server
            const issueId = getCurrentIssueId();
            const tokenUrl = '/api/voice/token' + (issueId ? `?issue=${issueId}` : '');
            const tokenResp = await fetch(tokenUrl);
            if (!tokenResp.ok) {
                const err = await tokenResp.json();
                throw new Error(err.detail || 'Token request failed');
            }
            const tokenData = await tokenResp.json();
            const ephemeralKey = tokenData.value || tokenData.client_secret?.value;
            if (!ephemeralKey) throw new Error('No ephemeral key in response');

            // 2. Create peer connection
            pc = new RTCPeerConnection();

            // 3. Set up remote audio playback
            audioEl = document.createElement('audio');
            audioEl.autoplay = true;
            pc.ontrack = (e) => { audioEl.srcObject = e.streams[0]; };

            // 4. Get microphone and add track
            localStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            pc.addTrack(localStream.getTracks()[0]);

            // 5. Create data channel for events
            dc = pc.createDataChannel('oai-events');
            dc.onopen = onDataChannelOpen;
            dc.onmessage = onDataChannelMessage;

            // 6. SDP offer/answer exchange
            const offer = await pc.createOffer();
            await pc.setLocalDescription(offer);

            const sdpResp = await fetch('https://api.openai.com/v1/realtime/calls?model=gpt-realtime', {
                method: 'POST',
                body: offer.sdp,
                headers: {
                    'Authorization': `Bearer ${ephemeralKey}`,
                    'Content-Type': 'application/sdp',
                },
            });

            if (!sdpResp.ok) throw new Error(`SDP exchange failed: ${sdpResp.status}`);

            const answerSdp = await sdpResp.text();
            await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp });

            pc.onconnectionstatechange = () => {
                if (pc.connectionState === 'connected') {
                    statusEl.textContent = 'Connected — speak naturally';
                    statusEl.className = 'voice-status active';
                    btn.className = 'voice-btn connected';
                    micIcon.style.display = 'none';
                    stopIcon.style.display = 'block';
                    log('Session started');
                } else if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected') {
                    stopVoice();
                    statusEl.textContent = 'Connection lost';
                    statusEl.className = 'voice-status error';
                }
            };

        } catch (err) {
            statusEl.textContent = 'Error: ' + err.message;
            statusEl.className = 'voice-status error';
            btn.className = 'voice-btn';
            stopVoice();
        }
    }

    function stopVoice() {
        if (dc) { try { dc.close(); } catch(e) {} dc = null; }
        if (localStream) {
            localStream.getTracks().forEach(t => t.stop());
            localStream = null;
        }
        if (pc) {
            pc.getSenders().forEach(s => { if (s.track) s.track.stop(); });
            try { pc.close(); } catch(e) {}
            pc = null;
        }
        if (audioEl) { audioEl.srcObject = null; audioEl = null; }
        btn.className = 'voice-btn';
        micIcon.style.display = 'block';
        stopIcon.style.display = 'none';
        statusEl.textContent = 'Tap to start a voice session';
        statusEl.className = 'voice-status';
        log('Session ended');
    }

    function onDataChannelOpen() {
        // Data channel is open — session is configured via the token
        log('Data channel open');
    }

    async function onDataChannelMessage(e) {
        const event = JSON.parse(e.data);

        // Handle function calls from the model
        if (event.type === 'response.done' && event.response?.output) {
            for (const output of event.response.output) {
                if (output.type === 'function_call') {
                    await handleToolCall(output);
                }
            }
        }
    }

    async function handleToolCall(output) {
        const fnName = output.name;
        const fnArgs = JSON.parse(output.arguments || '{}');
        log(`Tool: ${fnName}(${JSON.stringify(fnArgs)})`, 'tool-call');

        try {
            // Execute the tool call on our server
            const resp = await fetch('/api/voice/tool-call', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ name: fnName, arguments: fnArgs }),
            });
            const data = await resp.json();
            const result = data.result || 'No result';

            // Send the result back to OpenAI via data channel
            if (dc && dc.readyState === 'open') {
                // Create a conversation item with the function output
                dc.send(JSON.stringify({
                    type: 'conversation.item.create',
                    item: {
                        type: 'function_call_output',
                        call_id: output.call_id,
                        output: result,
                    },
                }));
                // Trigger the model to continue responding
                dc.send(JSON.stringify({ type: 'response.create' }));
            }

            log(`Result: ${result.substring(0, 100)}...`, 'tool-call');
        } catch (err) {
            log(`Tool error: ${err.message}`, 'tool-call');
        }
    }

    // Keep screen awake
    let wakeLock = null;
    async function requestWakeLock() {
        try {
            if ('wakeLock' in navigator) {
                wakeLock = await navigator.wakeLock.request('screen');
            }
        } catch (e) { /* ignore */ }
    }
    requestWakeLock();
    document.addEventListener('visibilitychange', () => {
        if (document.visibilityState === 'visible') requestWakeLock();
    });
    {% endif %}
    </script>
</body>
</html>
